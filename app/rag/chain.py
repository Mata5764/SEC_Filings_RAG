from typing import TypedDict, Annotated, Sequence
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
import operator
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

# Define the state
class AgentState(TypedDict):
    """State for the RAG agent."""
    messages: Annotated[Sequence[HumanMessage | AIMessage], operator.add]
    context: str
    question: str

def retrieve(state: AgentState) -> AgentState:
    """
    Retrieve relevant documents and update the state.
    This function handles all retrieval-related operations including:
    - Vector store initialization
    - Document retrieval
    - Context preparation
    
    Args:
        state: Current state of the agent
        
    Returns:
        Updated state with retrieved context
    """
    # Ensure messages is a non-empty list
    messages = state.get("messages")
    if not messages or not isinstance(messages, list):
        raise ValueError("AgentState['messages'] must be a non-empty list of messages.")

    # Initialize embeddings
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        show_progress_bar=False
    )
    
    # Initialize vector store and retriever
    vectorstore = Chroma(
        persist_directory="app/data/chroma_db",
        embedding_function=embeddings
    )
    
    retriever = vectorstore.as_retriever(
        search_kwargs={
            "k": 5,  # Number of documents to retrieve
            "filter": None  # Can add filters if needed
        }
    )
    
    # Get the latest question
    question = messages[-1].content
    
    # Retrieve relevant documents
    docs = retriever.invoke(question)
    
    # Join document contents
    context = "\n".join(doc.page_content for doc in docs)
    
    # Update state with retrieved context and question
    return {
        "messages": messages,
        "context": context,
        "question": question
    }

def generate(state: AgentState) -> AgentState:
    """
    Generate an answer using the retrieved context.
    This function handles all LLM-related operations including:
    - Prompt template definition
    - LLM initialization
    - Response generation and formatting
    
    Args:
        state: Current state of the agent
        
    Returns:
        Updated state with the generated answer
    """
    # Define the system prompt
    system_prompt = """You are a financial analyst tasked with summarizing sections of a financial report.

Below is the context from the report that has been identified as relevant. 
Summarize only the meaningful insights from the context, with a focus on financial performance, strategic direction, or risk.

Instructions:
- Summarize clearly in 5-6 sentences.
- Do not include irrelevant details.
- Preserve key facts or figures.
- Rephrase in a professional, executive tone.
"""

    # Initialize the prompt with messages
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "Documents:\n{context}\n\nUser question:\n{question}")
    ])
    
    # Initialize the LLM
    llm = ChatOpenAI(model="gpt-4-turbo")
    
    # Generate answer using the context
    response = llm.invoke(
        prompt.format(
            context=state["context"],
            question=state["question"]
        )
    )
    
    # Add the response to messages
    return {
        "messages": state["messages"] + [AIMessage(content=response.content)],
        "context": state["context"],
        "question": state["question"]
    }

# Create the graph
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)

# Set the entry point
workflow.set_entry_point("retrieve")

# Add edges
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

# Compile the graph
rag_chain = workflow.compile()

def query_rag(question: str) -> str:
    """
    Query the RAG pipeline with a question.
    
    Args:
        question (str): The question to ask
        
    Returns:
        str: The answer generated by the RAG pipeline
    """
    try:
        # Initialize the state
        state = {
            "messages": [HumanMessage(content=question)],
            "context": "",
            "question": question
        }
        
        # Run the graph
        result = rag_chain.invoke(state)
        
        # Return the last message (the answer)
        return result["messages"][-1].content
    except Exception as e:
        return f"Error processing your question: {str(e)}"
